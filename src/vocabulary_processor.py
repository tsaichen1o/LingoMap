#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# python src/vocabulary_processor.py
"""
LingoMap Vocabulary Processor (v2 - Cloud Ready)
Reads vocabulary from Turtle files, generates embeddings, and stores them in ChromaDB.
Prioritizes ChromaDB Cloud connection and falls back to local persistence if not configured.
"""
import sys
import os

# Fix SQLite version issue for Streamlit Cloud deployment
try:
    import pysqlite3
    sys.modules['sqlite3'] = pysqlite3
except ImportError:
    pass

import chromadb
from rdflib import Graph, RDFS, SKOS, URIRef, Namespace
from sentence_transformers import SentenceTransformer
from typing import List, Dict
from dotenv import load_dotenv

load_dotenv()

class VocabularyProcessor:
    """
    Processes RDF vocabulary files into a searchable vector database.
    """

    def __init__(self, model_name: str = 'all-mpnet-base-v2', quota_limit: int = 128):
        print(f"ğŸš€ Initializing Vocabulary Processor...")
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (åªåœ¨éœ€è¦æ™‚ä¸‹è¼‰ä¸€æ¬¡)
        print(f"   - Loading embedding model: {model_name}...")
        self.embedding_model = SentenceTransformer(model_name)
        
        # è¨­å®šé…é¡é™åˆ¶
        self.quota_limit = quota_limit
        print(f"   - ChromaDB quota limit set to: {quota_limit} bytes")
        
        # --- [MODIFIED] åˆå§‹åŒ– ChromaDBï¼Œå„ªå…ˆä½¿ç”¨é›²ç«¯ç‰ˆæœ¬ ---
        print("   - Initializing ChromaDB client...")
        
        # å¾ç’°å¢ƒè®Šæ•¸ä¸­è®€å–é›²ç«¯è¨­å®š
        chroma_api_key = os.getenv("CHROMADB_API_KEY")
        chroma_tenant = os.getenv("CHROMADB_TENANT_ID")
        chroma_database = os.getenv("CHROMADB_NAME")
        
        # æª¢æŸ¥æ˜¯å¦æ‰€æœ‰å¿…è¦çš„é›²ç«¯è®Šæ•¸éƒ½å·²è¨­å®š
        if chroma_api_key and chroma_tenant and chroma_database:
            print("   - ç™¼ç¾é›²ç«¯è¨­å®šï¼Œæ­£åœ¨å˜—è©¦é€£æ¥åˆ° ChromaDB Cloud...")
            try:
                self.client = chromadb.CloudClient(
                    tenant=chroma_tenant,
                    database=chroma_database,
                    api_key=chroma_api_key
                )
                print("   âœ… æˆåŠŸé€£æ¥åˆ° ChromaDB Cloud!")
            except Exception as e:
                print(f"   âŒ é€£æ¥åˆ° ChromaDB Cloud å¤±æ•—: {e}")
                print("   - å°‡é€€å›ä½¿ç”¨æœ¬æ©Ÿç«¯å„²å­˜ã€‚")
                self._init_local_client()
        else:
            print("   - æœªæ‰¾åˆ°å®Œæ•´çš„ ChromaDB Cloud è¨­å®šï¼Œå°‡ä½¿ç”¨æœ¬æ©Ÿç«¯å„²å­˜ã€‚")
            self._init_local_client()

        # 3. å‰µå»ºæˆ–ç²å–ä¸€å€‹ Collection (æ­¤éƒ¨åˆ†é‚è¼¯ä¸è®Š)
        collection_name = "lingomap_vocab"
        print(f"   - Getting or creating collection: {collection_name}")
        self.collection = self.client.get_or_create_collection(name=collection_name)

        # 4. è·Ÿè¸ªå·²å¤„ç†çš„IDï¼Œé¿å…é‡å¤
        print("   - Fetching existing IDs from the collection to prevent duplicates...")
        try:
            # ä½¿ç”¨ get() æ–¹æ³•ï¼Œåªè«‹æ±‚ idï¼Œé€™æ¨£å¯ä»¥è™•ç†å¤§é‡çš„ç¾æœ‰é …ç›®
            existing_items = self.collection.get(include=[]) 
            self.processed_ids = set(existing_items['ids'])
            print(f"   - Found {len(self.processed_ids)} existing IDs in the collection.")
        except Exception as e:
            print(f"   - Warning: Could not fetch existing IDs. May re-process some items. Error: {e}")
            self.processed_ids = set()
        print("âœ… Initialization complete.")
        
    def _init_local_client(self):
        """ä¸€å€‹è¼”åŠ©å‡½å¼ï¼Œç”¨æ–¼åˆå§‹åŒ–æœ¬æ©Ÿç«¯çš„ ChromaDB Clientã€‚"""
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(script_dir)
        db_path = os.path.join(project_root, "lingomap_db")
        self.client = chromadb.PersistentClient(path=db_path)

    def process_turtle_file(self, file_path: str):
        """
        Reads a Turtle file, extracts terms and a RICH set of descriptions, 
        and adds them to the database.
        """
        if not os.path.exists(file_path):
            print(f"âŒ Error: File not found at {file_path}")
            return

        print(f"\nğŸ“„ Processing file: {file_path}...")
        g = Graph()
        # ... (è§£ææª”æ¡ˆçš„é‚è¼¯ç¶­æŒä¸è®Š) ...
        file_ext = os.path.splitext(file_path)[1].lower()
        parse_format = "turtle" if file_ext == '.ttl' else "xml"
        try:
            g.parse(file_path, format=parse_format)
        except Exception as e:
            print(f"   - âŒ Error parsing file {file_path}: {str(e)}")
            return

        documents_to_add, metadatas_to_add, ids_to_add = [], [], []
        skipped_count = 0

        # --- æ–°çš„è¼”åŠ©å‡½å¼ï¼Œç”¨ä¾†ç²å–é—œè¯è³‡æºçš„æ¨™ç±¤ ---
        def get_label(subject, g):
            """Helper to get the label of a subject in the graph."""
            label = g.value(subject, RDFS.label) or g.value(subject, SKOS.prefLabel)
            return str(label) if label else None

        for subject in g.subjects():
            if isinstance(subject, URIRef):
                subject_str = str(subject)
                if subject_str in self.processed_ids:
                    skipped_count += 1
                    continue

                label = get_label(subject, g)
                definition = g.value(subject, SKOS.definition) or g.value(subject, RDFS.comment)

                if label and definition:
                    # --- æ ¸å¿ƒä¿®æ”¹ï¼šå»ºç«‹ä¸€å€‹è±å¯Œçš„ä¸Šä¸‹æ–‡åˆ—è¡¨ ---
                    context_parts = []
                    
                    # 1. ç²å–çˆ¶å±¬æ€§/çˆ¶é¡åˆ¥çš„åç¨±
                    for super_prop in g.objects(subject, RDFS.subPropertyOf):
                        super_label = get_label(super_prop, g)
                        if super_label:
                            context_parts.append(f"Is a type of: {super_label}")

                    for super_class in g.objects(subject, RDFS.subClassOf):
                        super_label = get_label(super_class, g)
                        if super_label:
                             context_parts.append(f"Is a subclass of: {super_label}")
                    
                    # 2. ç²å–å€¼åŸŸ(range)çš„åç¨±
                    for range_class in g.objects(subject, RDFS.range):
                        range_label = get_label(range_class, g)
                        if range_label:
                            context_parts.append(f"Expected data type: {range_label}")

                    # 3. ç²å–å…¶ä»–ç­†è¨˜
                    # (æ³¨æ„ï¼šæ‚¨éœ€è¦ç¶å®š cmns-av namespace æ‰èƒ½ç”¨ cmns-av:usageNote)
                    CMNS_AV = Namespace("https://www.omg.org/spec/Commons/AnnotationVocabulary/")
                    for note in g.objects(subject, CMNS_AV.usageNote):
                        context_parts.append(f"Usage note: {str(note)}")
                    
                    # --- çµ„åˆæœ€çµ‚çš„ã€è±å¯Œçš„ã€Œèªç¾©æ–‡ä»¶ã€ ---
                    document_text = f"Term: {label}\nDefinition: {str(definition)}"
                    if context_parts:
                        document_text += "\n\nContext:\n- " + "\n- ".join(context_parts)
                    # ------------------------------------------

                    documents_to_add.append(document_text)
                    metadatas_to_add.append({
                        "uri": subject_str, "label": str(label), "source_file": os.path.basename(file_path)
                    })
                    ids_to_add.append(subject_str)
                    self.processed_ids.add(subject_str)

        # ... (å¾ŒçºŒåŠ å…¥ ChromaDB çš„é‚è¼¯ç¶­æŒä¸è®Š) ...
        if not documents_to_add:
            print("   - No suitable terms found.")
            return
        
        # --- [NEW] Smart Batching Logic with Dynamic Quota Management ---
        MAX_BATCH_ITEMS = 100  # æ‰¹æ¬¡æœ€å¤§é …ç›®æ•¸
        MAX_BATCH_BYTES = 100 * 1024  # æ‰¹æ¬¡æœ€å¤§IDä½å…ƒçµ„å¤§å° (100KBï¼Œå®‰å…¨èµ·è¦‹)
        
        # ChromaDB Cloud é…é¡é™åˆ¶ï¼ˆæ ¹æ“šéŒ¯èª¤è¨Šæ¯èª¿æ•´ï¼‰
        CHROMADB_QUOTA_LIMIT = self.quota_limit  # ä½¿ç”¨å¯¦ä¾‹è®Šæ•¸
        SAFETY_MARGIN = 0.8  # å®‰å…¨é‚Šéš›ï¼Œåªä½¿ç”¨ 80% çš„é…é¡
        
        print(f"   - Found {len(documents_to_add)} new terms. Generating rich embeddings...")
        
        current_batch_docs, current_batch_metas, current_batch_ids = [], [], []
        current_batch_bytes = 0
        total_processed = 0
        
        for i in range(len(documents_to_add)):
            doc, meta, doc_id = documents_to_add[i], metadatas_to_add[i], ids_to_add[i]
            id_bytes = len(doc_id.encode('utf-8'))
            
            # æª¢æŸ¥å–®å€‹ ID æ˜¯å¦è¶…éé…é¡é™åˆ¶
            if id_bytes > CHROMADB_QUOTA_LIMIT * SAFETY_MARGIN:
                print(f"     - âš ï¸ Skipping item with ID size {id_bytes} bytes (exceeds quota limit)")
                continue

            # æª¢æŸ¥åŠ å…¥é€™å€‹æ–°é …ç›®å¾Œï¼Œæ˜¯å¦æœƒè¶…éé…é¡é™åˆ¶
            if (current_batch_docs and 
               (len(current_batch_docs) >= MAX_BATCH_ITEMS or 
                current_batch_bytes + id_bytes > CHROMADB_QUOTA_LIMIT * SAFETY_MARGIN)):
                
                # æäº¤ç•¶å‰çš„æ‰¹æ¬¡
                if not self._submit_batch(current_batch_docs, current_batch_metas, current_batch_ids):
                    print(f"   - âŒ Batch submission failed. Stopping processing for this file.")
                    return # å¦‚æœæäº¤å¤±æ•—ï¼ˆä¾‹å¦‚å› ç‚ºé…é¡ï¼‰ï¼Œå‰‡çµ‚æ­¢æ­¤æª”æ¡ˆçš„è™•ç†
                
                total_processed += len(current_batch_docs)
                # é‡ç½®æ‰¹æ¬¡
                current_batch_docs, current_batch_metas, current_batch_ids = [], [], []
                current_batch_bytes = 0

            # å°‡ç•¶å‰é …ç›®åŠ å…¥åˆ°æ‰¹æ¬¡ä¸­
            current_batch_docs.append(doc)
            current_batch_metas.append(meta)
            current_batch_ids.append(doc_id)
            current_batch_bytes += id_bytes
        
        # æäº¤æœ€å¾Œå‰©é¤˜çš„æ‰¹æ¬¡
        if current_batch_docs:
            if self._submit_batch(current_batch_docs, current_batch_metas, current_batch_ids):
                total_processed += len(current_batch_docs)

        print(f"âœ… Successfully processed {total_processed} terms from {file_path} (skipped {len(documents_to_add) - total_processed} due to quota limits).")
    
    def _submit_batch(self, docs, metas, ids) -> bool:
        """Helper function to submit a single batch and handle errors. Returns True on success."""
        try:
            batch_size_bytes = sum(len(i.encode('utf-8')) for i in ids)
            print(f"     - Submitting batch with {len(docs)} items ({batch_size_bytes} bytes)...")
            
            # æª¢æŸ¥æ‰¹æ¬¡å¤§å°æ˜¯å¦è¶…éé…é¡
            if batch_size_bytes > self.quota_limit:  # ä½¿ç”¨å¯¦ä¾‹è®Šæ•¸
                print(f"     - âš ï¸ Batch size ({batch_size_bytes} bytes) exceeds quota limit ({self.quota_limit} bytes)")
                print(f"     - Reducing batch size and retrying...")
                
                # å‹•æ…‹æ¸›å°‘æ‰¹æ¬¡å¤§å°
                if len(docs) > 1:
                    # åˆ†æˆå…©å€‹è¼ƒå°çš„æ‰¹æ¬¡
                    mid = len(docs) // 2
                    batch1_success = self._submit_batch(docs[:mid], metas[:mid], ids[:mid])
                    batch2_success = self._submit_batch(docs[mid:], metas[mid:], ids[mid:])
                    return batch1_success and batch2_success
                else:
                    print(f"     - âŒ Single item too large ({batch_size_bytes} bytes). Skipping.")
                    return False
            
            self.collection.add(documents=docs, metadatas=metas, ids=ids)
            print(f"     - âœ… Batch added successfully.")
            self.processed_ids.update(ids)
            return True
            
        except chromadb.errors.ChromaError as e:
            if "Quota exceeded" in str(e):
                print(f"\n   - âŒ ChromaDB Cloud quota exceeded during batch submission.")
                print(f"   - Details: {e}")
                # å˜—è©¦æ¸›å°‘æ‰¹æ¬¡å¤§å°ä¸¦é‡è©¦
                if len(docs) > 1:
                    print(f"   - ğŸ”„ Attempting to split batch and retry...")
                    mid = len(docs) // 2
                    batch1_success = self._submit_batch(docs[:mid], metas[:mid], ids[:mid])
                    batch2_success = self._submit_batch(docs[mid:], metas[mid:], ids[mid:])
                    return batch1_success and batch2_success
                else:
                    print(f"   - âŒ Single item exceeds quota. Skipping.")
                    return False
            else:
                print(f"   - âŒ A ChromaDB error occurred: {e}")
                return False
        except Exception as e:
            print(f"   - âŒ An unexpected error occurred during batch add: {e}")
            return False

    def search(self, query_text: str, n_results: int = 5) -> List[Dict]:
        """
        Searches the vector database for the most similar terms.
        """
        print(f"\nğŸ” Searching for terms similar to: '{query_text}'...")
        results = self.collection.query(
            query_texts=[query_text],
            n_results=n_results
        )

        # æ¸…ç†ä¸¦å›å‚³çµæœ
        cleaned_results = []
        if results and results['metadatas'][0]:
            for i, metadata in enumerate(results['metadatas'][0]):
                cleaned_results.append({
                    "label": metadata.get("label"),
                    "uri": metadata.get("uri"),
                    "distance": results['distances'][0][i]  # è·é›¢è¶Šå°è¶Šç›¸ä¼¼
                })

        return cleaned_results


def main():
    """ä¸»å‡½æ•¸ï¼Œç”¨æ–¼å±•ç¤ºæ•´å€‹æµç¨‹"""

    # --- æ­¥é©Ÿ 2.1: å»ºç«‹å‘é‡è³‡æ–™åº« ---
    processor = VocabularyProcessor()

    # --- é€™æ˜¯ä¸»è¦çš„ä¿®æ”¹ä¹‹è™• ---
    # è‡ªå‹•æƒæ 'vocabularies' è³‡æ–™å¤¾ä¸‹çš„æ‰€æœ‰ .ttl å’Œ .rdf æª”æ¡ˆ
    # ä¿®å¤è·¯å¾„é—®é¢˜ï¼šç¡®ä¿æ— è®ºä»å“ªä¸ªç›®å½•è¿è¡Œéƒ½èƒ½æ‰¾åˆ°æ­£ç¡®çš„vocabulariesæ–‡ä»¶å¤¹
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)  # ä»srcç›®å½•å›åˆ°é¡¹ç›®æ ¹ç›®å½•
    vocab_directory = os.path.join(project_root, "vocabularies")

    ttl_files_to_process = []
    if not os.path.exists(vocab_directory):
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° '{vocab_directory}' è³‡æ–™å¤¾ã€‚è«‹å…ˆå»ºç«‹å®ƒä¸¦æ”¾å…¥è©å½™åº«æª”æ¡ˆã€‚")
        return

    for root, _, files in os.walk(vocab_directory):
        for file in files:
            if file.endswith((".ttl", ".rdf", ".owl")):
                full_path = os.path.join(root, file)
                ttl_files_to_process.append(full_path)

    if not ttl_files_to_process:
        print(f"âš ï¸ è­¦å‘Šï¼šåœ¨ '{vocab_directory}' è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•è©å½™åº«æª”æ¡ˆã€‚")
        # å³ä½¿æ²’æœ‰æ‰¾åˆ°æª”æ¡ˆï¼Œæˆ‘å€‘ä¾ç„¶å¯ä»¥ç¹¼çºŒåŸ·è¡Œæœç´¢ï¼Œåªæ˜¯çŸ¥è­˜åº«æ˜¯ç©ºçš„
    else:
        print(f"ğŸ“‚ ç™¼ç¾ {len(ttl_files_to_process)} å€‹è©å½™åº«æª”æ¡ˆï¼Œæº–å‚™è™•ç†...")

        for ttl_file in ttl_files_to_process:
            processor.process_turtle_file(ttl_file)
    # --- ä¿®æ”¹çµæŸ ---

    # --- æ­¥é©Ÿ 2.2: è¨­è¨ˆæª¢ç´¢é‚è¼¯ (ç¶­æŒä¸è®Š) ---

    # æ¨¡æ“¬ç•¶ç”¨æˆ¶é¸æ“‡äº† 'CERT' æ¬„ä½
    column_name = "CERT"
    sample_data = [14761.0, 57899.0]
    query = f"Column Name: {column_name}\nDescription: A certificate number for a financial institution.\nSample Values: {sample_data}"

    search_results = processor.search(query, n_results=5)

    print("\nğŸ“Š Search Results:")
    if search_results:
        for result in search_results:
            print(
                f"  - Label: {result['label']}\n    URI: {result['uri']}\n    Distance: {result['distance']:.4f}\n")
    else:
        print("   - No results found.")


if __name__ == "__main__":
    main()
